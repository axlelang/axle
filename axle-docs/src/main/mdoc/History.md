---
layout: page
title: History
permalink: /history/
---

## αχλε

Axle models a set of formal subjects that the author has encountered throughout his lifetime.
They take the form of functioning code that allows the reader to experiment with alternative examples.

Although the primary aim of this code is education and clarity, scalability and performance
are secondary goals.

The name "axle" was originally chosen because it sounds like "Haskell".
Given the use of UTF symbols, I tried spelling it using Greek letters to get "αχλε".
It turns out that this is the Etruscan spelling of
[Achilles](http://en.wikipedia.org/wiki/Achilles)

![Achilles](https://upload.wikimedia.org/wikipedia/commons/4/4c/Etruscan_mural_achilles_Troilus.gif)

([image context](http://en.wikipedia.org/wiki/File:Etruscan_mural_achilles_Troilus.gif))

Follow [@axledsl](https://twitter.com/axledsl) on Twitter.

## References

### Quanta

The first time I had the idea to group units into quanta was at NOCpulse (2000-2002).
NOCpulse was bought by Red Hat, which open-sourced the code.
There is still [evidence](http://spacewalk.redhat.com/documentation/schema-doc/sql_sources/table/rhn_quanta.sql) of that early code online.

In a 2006 class given by [Alan Kay](http://en.wikipedia.org/wiki/Alan_Kay) at UCLA,
I proposed a system for exploring and learning about scale.
The idea occurred to me after reading a news article about a new rocket engine that used the Hoover Dam as a
point of reference.
I wound up implementing another idea, but always meant to come back to it.

### Machine Learning

Based on many classes at Stanford (in the 90's) and UCLA (in the 00's),
and more recently the Coursera machine learning course in the Fall of 2011.
The inimitable [Artificial Intelligence: A Modern Approach](http://www.amazon.com/Artificial-Intelligence-Modern-Approach-Edition/dp/0136042597/ref=sr_1_1?ie=UTF8) has been a mainstay throughout.

### Statistics, Information Theory, Bayesian Networks, &amp; Causality

The Information Theory code is based on [Thomas Cover](https://en.wikipedia.org/wiki/Thomas_M._Cover)'s
[Elements of Information Theory](http://www.amazon.com/Elements-Information-Theory-Telecommunications-Processing/dp/0471241954/ref=sr_1_1?ie=UTF8)
and his EE 376A course.

I implemented some Bayesian Networks code in Java around 2006 while
[Adnan Darwiche](http://www.cs.ucla.edu/~darwiche/) class on the subject at UCLA.
The Axle version is based on his book,
[Modeling and Reasoning with Bayesian Networks](http://www.amazon.com/Modeling-Reasoning-Bayesian-Networks-Darwiche/dp/0521884381/ref=sr_1_1?ie=UTF8)

Similarly, I implemented ideas from [Judea Pearl](http://bayes.cs.ucla.edu/jp_home.html)
UCLA course on Causality in Java.
The Axle version is based on his classic text [Causality](http://www.amazon.com/Causality-Reasoning-Inference-Judea-Pearl/dp/052189560X/ref=sr_1_1?ie=UTF8)

### Game Theory

As a senior CS major at Stanford in 1996, I did some
independent research with
Professor [Daphne Koller](http://ai.stanford.edu/~koller/) and
PhD student [Avi Pfeffer](http://www.gelberpfeffer.net/avi).

This work spanned two quarters.
The first quarter involved using Koller and Pfeffer's
[Gala](http://ai.stanford.edu/~koller/Papers/Koller+Pfeffer:AIJ97.pdf) language
(a Prolog-based DSL for describing games) to study a small
version of Poker and solve for the [Nash equilibria](http://en.wikipedia.org/wiki/Nash_equilibrium).
The second (still unfinished) piece was to extend the
solver to handle non-zero-sum games.

The text I was using at the time was
[Eric Rasmusen](http://www.rasmusen.org/)'s
[Games and Information](http://www.amazon.com/Games-Information-Introduction-Game-Theory/dp/1405136669/ref=sr_1_1?ie=UTF8)

### Linguistics

Based on notes from [Ed Stabler](http://www.linguistics.ucla.edu/people/stabler/)'s
graduate courses on language evolution and
computational linguistics (Lx 212 08) at UCLA.

### Author

See the [author](/author/) page for more about the author.
